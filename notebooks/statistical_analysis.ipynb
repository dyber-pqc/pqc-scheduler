{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PQC Scheduler Statistical Analysis\n",
    "\n",
    "Statistical analysis of benchmark results for the PQC Scheduler.\n",
    "\n",
    "Copyright (c) 2025 Dyber, Inc. All Rights Reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Benchmark Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(results_dir):\n",
    "    \"\"\"Load all benchmark results from a directory.\"\"\"\n",
    "    results = []\n",
    "    for f in sorted(Path(results_dir).glob('run_*.json')):\n",
    "        with open(f) as fp:\n",
    "            data = json.load(fp)\n",
    "            data['run_id'] = int(f.stem.split('_')[1])\n",
    "            results.append(data)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Load results - update path as needed\n",
    "RESULTS_DIR = '../results/benchmark_latest'\n",
    "df = load_results(RESULTS_DIR)\n",
    "print(f\"Loaded {len(df)} runs\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_metrics = [\n",
    "    'throughput_mean',\n",
    "    'latency_mean_us',\n",
    "    'latency_p99_us',\n",
    "    'sla_compliance',\n",
    "    'security_score'\n",
    "]\n",
    "\n",
    "stats_df = df[key_metrics].describe()\n",
    "stats_df.loc['cv'] = df[key_metrics].std() / df[key_metrics].mean()  # Coefficient of variation\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welch's t-test for Throughput Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def welch_ttest(group1, group2, alpha=0.05):\n",
    "    \"\"\"Perform Welch's t-test with effect size calculation.\"\"\"\n",
    "    t_stat, p_value = stats.ttest_ind(group1, group2, equal_var=False)\n",
    "    \n",
    "    # Cohen's d effect size\n",
    "    pooled_std = np.sqrt((np.var(group1) + np.var(group2)) / 2)\n",
    "    cohens_d = (np.mean(group1) - np.mean(group2)) / pooled_std\n",
    "    \n",
    "    # Degrees of freedom (Welch-Satterthwaite)\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    v1, v2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
    "    df = ((v1/n1 + v2/n2)**2) / ((v1/n1)**2/(n1-1) + (v2/n2)**2/(n2-1))\n",
    "    \n",
    "    return {\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'degrees_of_freedom': df,\n",
    "        'cohens_d': cohens_d,\n",
    "        'significant': p_value < alpha\n",
    "    }\n",
    "\n",
    "# Example: Compare MDP-Optimal vs Static-PQC (simulated)\n",
    "# In real analysis, load comparison data from different configurations\n",
    "mdp_throughput = df['throughput_mean'].values\n",
    "baseline_throughput = mdp_throughput * 0.807  # 23.7% improvement simulation\n",
    "\n",
    "result = welch_ttest(mdp_throughput, baseline_throughput)\n",
    "print(\"Welch's t-test Results:\")\n",
    "for k, v in result.items():\n",
    "    print(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(data, n_bootstrap=10000, ci=0.95, statistic=np.mean):\n",
    "    \"\"\"Compute bootstrap confidence interval using BCa method.\"\"\"\n",
    "    data = np.array(data)\n",
    "    n = len(data)\n",
    "    \n",
    "    # Bootstrap samples\n",
    "    boot_stats = np.array([\n",
    "        statistic(np.random.choice(data, size=n, replace=True))\n",
    "        for _ in range(n_bootstrap)\n",
    "    ])\n",
    "    \n",
    "    # BCa correction\n",
    "    theta_hat = statistic(data)\n",
    "    \n",
    "    # Bias correction\n",
    "    z0 = stats.norm.ppf(np.mean(boot_stats < theta_hat))\n",
    "    \n",
    "    # Acceleration (jackknife)\n",
    "    jack_stats = np.array([\n",
    "        statistic(np.delete(data, i)) for i in range(n)\n",
    "    ])\n",
    "    jack_mean = np.mean(jack_stats)\n",
    "    a = np.sum((jack_mean - jack_stats)**3) / (6 * np.sum((jack_mean - jack_stats)**2)**1.5)\n",
    "    \n",
    "    # Adjusted percentiles\n",
    "    alpha = (1 - ci) / 2\n",
    "    z_alpha = stats.norm.ppf(alpha)\n",
    "    z_1_alpha = stats.norm.ppf(1 - alpha)\n",
    "    \n",
    "    alpha1 = stats.norm.cdf(z0 + (z0 + z_alpha) / (1 - a * (z0 + z_alpha)))\n",
    "    alpha2 = stats.norm.cdf(z0 + (z0 + z_1_alpha) / (1 - a * (z0 + z_1_alpha)))\n",
    "    \n",
    "    ci_lower = np.percentile(boot_stats, alpha1 * 100)\n",
    "    ci_upper = np.percentile(boot_stats, alpha2 * 100)\n",
    "    \n",
    "    return {\n",
    "        'mean': theta_hat,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'ci_width': ci_upper - ci_lower\n",
    "    }\n",
    "\n",
    "# Compute CIs for key metrics\n",
    "print(\"95% Bootstrap Confidence Intervals (BCa):\")\n",
    "print(\"=\" * 60)\n",
    "for metric in key_metrics:\n",
    "    ci = bootstrap_ci(df[metric].values)\n",
    "    print(f\"{metric}:\")\n",
    "    print(f\"  Mean: {ci['mean']:.4f}\")\n",
    "    print(f\"  95% CI: [{ci['ci_lower']:.4f}, {ci['ci_upper']:.4f}]\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonferroni Correction for Multiple Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonferroni_correction(p_values, alpha=0.05):\n",
    "    \"\"\"Apply Bonferroni correction for multiple hypothesis testing.\"\"\"\n",
    "    n_tests = len(p_values)\n",
    "    adjusted_alpha = alpha / n_tests\n",
    "    \n",
    "    results = []\n",
    "    for name, p in p_values.items():\n",
    "        results.append({\n",
    "            'test': name,\n",
    "            'p_value': p,\n",
    "            'adjusted_alpha': adjusted_alpha,\n",
    "            'significant': p < adjusted_alpha\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example with multiple comparisons\n",
    "p_values = {\n",
    "    'throughput': 1e-10,\n",
    "    'latency': 2e-8,\n",
    "    'sla_compliance': 5e-12,\n",
    "    'security': 3e-6,\n",
    "    'algorithm_switches': 1e-4,\n",
    "    'fallback_events': 2e-3\n",
    "}\n",
    "\n",
    "corrected = bonferroni_correction(p_values, alpha=0.01)\n",
    "print(\"Bonferroni Corrected Results (α=0.01):\")\n",
    "corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect Size Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_cohens_d(d):\n",
    "    \"\"\"Interpret Cohen's d effect size.\"\"\"\n",
    "    d = abs(d)\n",
    "    if d < 0.2:\n",
    "        return 'negligible'\n",
    "    elif d < 0.5:\n",
    "        return 'small'\n",
    "    elif d < 0.8:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'large'\n",
    "\n",
    "# Effect sizes from paper\n",
    "effect_sizes = {\n",
    "    'MDP vs Static-PQC Throughput': 2.14,\n",
    "    'Graceful Degradation Switch Time': 3.47,\n",
    "    'Phased vs Big-Bang Migration Risk': 2.47\n",
    "}\n",
    "\n",
    "print(\"Effect Size Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "for name, d in effect_sizes.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Cohen's d: {d:.2f}\")\n",
    "    print(f\"  Interpretation: {interpret_cohens_d(d)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_table(df):\n",
    "    \"\"\"Generate publication-ready summary table.\"\"\"\n",
    "    summary = []\n",
    "    \n",
    "    for metric in key_metrics:\n",
    "        values = df[metric].values\n",
    "        ci = bootstrap_ci(values)\n",
    "        \n",
    "        summary.append({\n",
    "            'Metric': metric.replace('_', ' ').title(),\n",
    "            'Mean ± Std': f\"{np.mean(values):.2f} ± {np.std(values):.2f}\",\n",
    "            '95% CI': f\"[{ci['ci_lower']:.2f}, {ci['ci_upper']:.2f}]\",\n",
    "            'Min': f\"{np.min(values):.2f}\",\n",
    "            'Max': f\"{np.max(values):.2f}\"\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "summary_table = generate_summary_table(df)\n",
    "print(\"Summary Table for Publication:\")\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to LaTeX\n",
    "latex_table = summary_table.to_latex(index=False, escape=False)\n",
    "print(\"\\nLaTeX Table:\")\n",
    "print(latex_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
